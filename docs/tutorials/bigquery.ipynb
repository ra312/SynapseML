{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow IO Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# End to end example for BigQuery TensorFlow reader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/io/tutorials/bigquery\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/bigquery.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/io/blob/master/docs/tutorials/bigquery.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/io/docs/tutorials/bigquery.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial shows how to use [BigQuery TensorFlow reader](https://github.com/tensorflow/io/tree/master/tensorflow_io/bigquery) for training neural network using the Keras sequential API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WodUv8O1VKmr"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This tutorial uses the [United States Census Income\n",
        "Dataset](https://archive.ics.uci.edu/ml/datasets/census+income) provided by the\n",
        "[UC Irvine Machine Learning\n",
        "Repository](https://archive.ics.uci.edu/ml/index.php). This dataset contains\n",
        "information about people from a 1994 Census database, including age, education,\n",
        "marital status, occupation, and whether they make more than $50,000 a year."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YsfgDMZW5g6"
      },
      "source": [
        "Set up your GCP project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a GCP project.](https://console.cloud.google.com/cloud-resource-manager)\n",
        "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "3. [Enable the BigQuery Storage API](https://cloud.google.com/bigquery/docs/reference/storage/#enabling_the_api)\n",
        "4. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "Note: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgCc3gXybsA"
      },
      "source": [
        "Install required Packages, and restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QeBQuayhuvhg"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # Use the Colab's preinstalled TensorFlow 2.x\n",
        "  %tensorflow_version 2.x \n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Tu01THzWcE-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d61d461c-30b2-4a7c-9f93-81bc001d8097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastavro\n",
            "  Downloading fastavro-1.4.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 9.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: fastavro\n",
            "Successfully installed fastavro-1.4.7\n",
            "Collecting tensorflow-io==0.9.0\n",
            "  Downloading tensorflow_io-0.9.0-cp37-cp37m-manylinux2010_x86_64.whl (41.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.6 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow<2.1.0,>=2.0.0\n",
            "  Downloading tensorflow-2.0.4-cp37-cp37m-manylinux2010_x86_64.whl (86.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 86.4 MB 95 kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (0.37.0)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (3.17.3)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (0.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.1.2)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "  Downloading tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n",
            "\u001b[K     |████████████████████████████████| 449 kB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.13.3)\n",
            "Collecting h5py<=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 50.1 MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.1.0,>=2.0.0\n",
            "  Downloading tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 35.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.42.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (3.3.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (3.1.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=e9b818cca4682431885e73313d63ca56dc36e410a0135b0cf26d0fe15fb6f591\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: numpy, h5py, tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow, tensorflow-io\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 h5py-2.10.0 keras-applications-1.0.8 numpy-1.18.5 tensorboard-2.0.2 tensorflow-2.0.4 tensorflow-estimator-2.0.1 tensorflow-io-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install fastavro\n",
        "!pip install tensorflow-io==0.9.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YUj0878jPyz7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed39e9f-24be-48fc-8495-666d35cf33f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-bigquery-storage in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery-storage) (1.26.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (21.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (3.17.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (2018.9)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (1.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (1.53.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (1.42.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (4.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (3.0.6)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigquery-storage) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-cloud-bigquery-storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZmI7l_GykcW"
      },
      "source": [
        "Authenticate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tUo3GJNxxZbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0055f6b-4a77-4f8a-8cab-def255149314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czF1KlC6y8fB"
      },
      "source": [
        "Set your PROJECT ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fbQR-bT_xgba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89fcf9ae-cd24-4598-9c09-b27609659297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "env: GCLOUD_PROJECT=cmp-development\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ID = \"cmp-development\" #@param {type:\"string\"}\n",
        "! gcloud config set project $PROJECT_ID\n",
        "%env GCLOUD_PROJECT=$PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOqN91M4y_9Y"
      },
      "source": [
        "Import Python libraries, define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "G3p2ICG80Z1t"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "from six.moves import urllib\n",
        "import tempfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import GoogleAPIError\n",
        "\n",
        "LOCATION = 'us'\n",
        "\n",
        "# Storage directory\n",
        "# DATA_DIR = os.path.join(tempfile.gettempdir(), 'census_data')\n",
        "\n",
        "# Download options.\n",
        "# DATA_URL = 'https://storage.googleapis.com/cloud-samples-data/ml-engine/census/data'\n",
        "# TRAINING_FILE = 'adult.data.csv'\n",
        "# EVAL_FILE = 'adult.test.csv'\n",
        "# TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
        "# EVAL_URL = '%s/%s' % (DATA_URL, EVAL_FILE)\n",
        "\n",
        "DATASET_ID = 'Preprocessing'\n",
        "TRAINING_TABLE_ID = 'DatasetTrain'\n",
        "# EVAL_TABLE_ID = 'census_eval_table'\n",
        "\n",
        "CSV_SCHEMA = [\n",
        "      bigquery.SchemaField(\"age\", \"FLOAT64\"),\n",
        "      bigquery.SchemaField(\"workclass\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"fnlwgt\", \"FLOAT64\"),\n",
        "      bigquery.SchemaField(\"education\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"education_num\", \"FLOAT64\"),\n",
        "      bigquery.SchemaField(\"marital_status\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"occupation\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"relationship\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"race\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"gender\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"capital_gain\", \"FLOAT64\"),\n",
        "      bigquery.SchemaField(\"capital_loss\", \"FLOAT64\"),\n",
        "      bigquery.SchemaField(\"hours_per_week\", \"FLOAT64\"),\n",
        "      bigquery.SchemaField(\"native_country\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"income_bracket\", \"STRING\"),\n",
        "  ]\n",
        "\n",
        "UNUSED_COLUMNS = [\"fnlwgt\", \"education_num\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooBfVnd-Xxd-"
      },
      "source": [
        "## Import census data into BigQuery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qt6wUD_2XFT"
      },
      "source": [
        "Define helper methods to load data into BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7mMI7uW_2vP5"
      },
      "outputs": [],
      "source": [
        "def create_bigquery_dataset_if_necessary(dataset_id):\n",
        "  # Construct a full Dataset object to send to the API.\n",
        "  client = bigquery.Client(project=PROJECT_ID)\n",
        "  dataset = bigquery.Dataset(bigquery.dataset.DatasetReference(PROJECT_ID, dataset_id))\n",
        "  dataset.location = LOCATION\n",
        "\n",
        "  try:\n",
        "    dataset = client.create_dataset(dataset)  # API request\n",
        "    return True\n",
        "  except GoogleAPIError as err:\n",
        "    if err.code != 409: # http_client.CONFLICT\n",
        "      raise\n",
        "  return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Y3Cr-DEfwNhK"
      },
      "outputs": [],
      "source": [
        "def load_data_into_bigquery(url, table_id):\n",
        "  create_bigquery_dataset_if_necessary(DATASET_ID)\n",
        "  client = bigquery.Client(project=PROJECT_ID)\n",
        "  dataset_ref = client.dataset(DATASET_ID)\n",
        "  table_ref = dataset_ref.table(table_id)\n",
        "  job_config = bigquery.LoadJobConfig()\n",
        "  job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "  job_config.source_format = bigquery.SourceFormat.CSV\n",
        "  job_config.schema = CSV_SCHEMA\n",
        "\n",
        "  load_job = client.load_table_from_uri(\n",
        "      url, table_ref, job_config=job_config\n",
        "  )\n",
        "  print(\"Starting job {}\".format(load_job.job_id))\n",
        "\n",
        "  load_job.result()  # Waits for table load to complete.\n",
        "  print(\"Job finished.\")\n",
        "\n",
        "  destination_table = client.get_table(table_ref)\n",
        "  print(\"Loaded {} rows.\".format(destination_table.num_rows))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSA0RIAZZEFZ"
      },
      "source": [
        "Load Census data in BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "wFZcK03-YDm4"
      },
      "outputs": [],
      "source": [
        "# load_data_into_bigquery(TRAINING_URL, TRAINING_TABLE_ID)\n",
        "# load_data_into_bigquery(EVAL_URL, EVAL_TABLE_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpUVI8IR2iXH"
      },
      "source": [
        "Confirm that data was imported\n",
        "\n",
        "TODO: replace \\<YOUR PROJECT\\> with your PROJECT_ID\n",
        "\n",
        "Note: --use_bqstorage_api will get data using BigQueryStorage API and will make sure that you are authorized to use it. Make sure that it is enabled for your project: https://cloud.google.com/bigquery/docs/reference/storage/#enabling_the_api\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "CVy3UkDgx2zi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6deebeea-8b5b-4a72-bc3d-50806a4b9c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing query with job ID: e9bb1306-1f31-4f5b-8fd2-deae8ddc2580\n",
            "\rQuery executing: 0.19s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:\n",
            " 404 Not found: Table cmp-development:Preprocessing.DatasetTrain was not found in location EU\n",
            "\n",
            "(job ID: e9bb1306-1f31-4f5b-8fd2-deae8ddc2580)\n",
            "\n",
            "                    -----Query Job SQL Follows-----                    \n",
            "\n",
            "    |    .    |    .    |    .    |    .    |    .    |    .    |\n",
            "   1:SELECT * FROM `cmp-development.Preprocessing.DatasetTrain` LIMIT 5\n",
            "    |    .    |    .    |    .    |    .    |    .    |    .    |\n"
          ]
        }
      ],
      "source": [
        "%%bigquery --use_bqstorage_api\n",
        "SELECT * FROM `cmp-development.Preprocessing.DatasetTrain` LIMIT 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "k4dhGnuYAfa_",
        "outputId": "48dd5b57-1a08-44e7-fd0f-5d926af2ef99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOu-pCksYTtE"
      },
      "source": [
        "##Load census data in TensorFlow DataSet using BigQuery reader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_Gm8Mzh62yF"
      },
      "source": [
        "Read and transform cesnus data from BigQuery into TensorFlow DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "NgPd9w5m06In"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow_io.bigquery import BigQueryClient\n",
        "from tensorflow_io.bigquery import BigQueryReadSession\n",
        "  \n",
        "def transform_row(row_dict):\n",
        "  # Trim all string tensors\n",
        "  trimmed_dict = { column:\n",
        "                  (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
        "                  for (column,tensor) in row_dict.items()\n",
        "                  }\n",
        "  # Extract feature column\n",
        "  income_bracket = trimmed_dict.pop('income_bracket')\n",
        "  # Convert feature column to 0.0/1.0\n",
        "  income_bracket_float = tf.cond(tf.equal(tf.strings.strip(income_bracket), '>50K'), \n",
        "                 lambda: tf.constant(1.0), \n",
        "                 lambda: tf.constant(0.0))\n",
        "  return (trimmed_dict, income_bracket_float)\n",
        "\n",
        "def read_bigquery(table_name):\n",
        "  tensorflow_io_bigquery_client = BigQueryClient()\n",
        "  read_session = tensorflow_io_bigquery_client.read_session(\n",
        "      \"projects/\" + PROJECT_ID,\n",
        "      PROJECT_ID, table_name, DATASET_ID,\n",
        "      list(field.name for field in CSV_SCHEMA \n",
        "           if not field.name in UNUSED_COLUMNS),\n",
        "      list(dtypes.double if field.field_type == 'FLOAT64' \n",
        "           else dtypes.string for field in CSV_SCHEMA\n",
        "           if not field.name in UNUSED_COLUMNS),\n",
        "      requested_streams=2)\n",
        "  \n",
        "  dataset = read_session.parallel_read_rows()\n",
        "  transformed_ds = dataset.map(transform_row)\n",
        "  return transformed_ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4_NlkxZt1rwR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "269a14fd-c268-4e7b-f227-791f95b3197f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "PermissionDeniedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-feaaa0035a7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mTRAINING_TABLE_ID\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DatasetTrain'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtraining_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_bigquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINING_TABLE_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# eval_ds = read_bigquery(EVAL_TABLE_ID).batch(BATCH_SIZE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-03457e7e8f55>\u001b[0m in \u001b[0;36mread_bigquery\u001b[0;34m(table_name)\u001b[0m\n\u001b[1;32m     28\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCSV_SCHEMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m            if not field.name in UNUSED_COLUMNS),\n\u001b[0;32m---> 30\u001b[0;31m       requested_streams=2)\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_read_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_io/bigquery/python/ops/bigquery_api.py\u001b[0m in \u001b[0;36mread_session\u001b[0;34m(self, parent, project_id, table_id, dataset_id, selected_fields, output_types, row_restriction, requested_streams)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mselected_fields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselected_fields\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         row_restriction=row_restriction)\n\u001b[0m\u001b[1;32m    135\u001b[0m     return BigQueryReadSession(parent, project_id, table_id, dataset_id,\n\u001b[1;32m    136\u001b[0m                                \u001b[0mselected_fields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_restriction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mio_big_query_read_session\u001b[0;34m(client, parent, project_id, table_id, dataset_id, selected_fields, output_types, requested_streams, row_restriction, container, shared_name, name)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mPermissionDeniedError\u001b[0m: Error reading from Cloud BigQuery: request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/balsam-brands-po-integration' [Op:IO>BigQueryReadSession]"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "TRAINING_TABLE_ID='DatasetTrain'\n",
        "training_ds = read_bigquery(TRAINING_TABLE_ID).shuffle(10000).batch(BATCH_SIZE)\n",
        "# eval_ds = read_bigquery(EVAL_TABLE_ID).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_iXxNdSYsDO"
      },
      "source": [
        "##Define feature columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UClHDwcyhFky"
      },
      "outputs": [],
      "source": [
        "def get_categorical_feature_values(column):\n",
        "  query = 'SELECT DISTINCT TRIM({}) FROM `{}`.{}.{}'.format(column, PROJECT_ID, DATASET_ID, TRAINING_TABLE_ID)\n",
        "  client = bigquery.Client(project=PROJECT_ID)\n",
        "  dataset_ref = client.dataset(DATASET_ID)\n",
        "  job_config = bigquery.QueryJobConfig()\n",
        "  query_job = client.query(query, job_config=job_config)\n",
        "  result = query_job.to_dataframe()\n",
        "  return result.values[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "h9aAs1ZAtQr-"
      },
      "outputs": [],
      "source": [
        "from tensorflow import feature_column\n",
        "\n",
        "feature_columns = []\n",
        "\n",
        "# numeric cols\n",
        "for header in ['capital_gain', 'capital_loss', 'hours_per_week']:\n",
        "  feature_columns.append(feature_column.numeric_column(header))\n",
        "\n",
        "# categorical cols\n",
        "for header in ['workclass', 'marital_status', 'occupation', 'relationship',\n",
        "               'race', 'native_country', 'education']:\n",
        "  categorical_feature = feature_column.categorical_column_with_vocabulary_list(\n",
        "        header, get_categorical_feature_values(header))\n",
        "  categorical_feature_one_hot = feature_column.indicator_column(categorical_feature)\n",
        "  feature_columns.append(categorical_feature_one_hot)\n",
        "\n",
        "# bucketized cols\n",
        "age = feature_column.numeric_column('age')\n",
        "age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
        "feature_columns.append(age_buckets)\n",
        "\n",
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfO0bhXXY3GQ"
      },
      "source": [
        "##Build and train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnKZKOQX7Qwx"
      },
      "source": [
        "Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Sm-UsB5_zvt0"
      },
      "outputs": [],
      "source": [
        "Dense = tf.keras.layers.Dense\n",
        "model = tf.keras.Sequential(\n",
        "  [\n",
        "    feature_layer,\n",
        "      Dense(100, activation=tf.nn.relu, kernel_initializer='uniform'),\n",
        "      Dense(75, activation=tf.nn.relu),\n",
        "      Dense(50, activation=tf.nn.relu),\n",
        "      Dense(25, activation=tf.nn.relu),\n",
        "      Dense(1, activation=tf.nn.sigmoid)\n",
        "  ])\n",
        "\n",
        "# Compile Keras model\n",
        "model.compile(\n",
        "    loss='binary_crossentropy', \n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8bSDfQd7T1n"
      },
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gPKrlFCN1y00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef84a52-34db-45af-a7ab-6a2a9de55437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer sequential is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4331: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "Epoch 1/5\n",
            "1018/1018 [==============================] - 14s 14ms/step - loss: 0.4736 - accuracy: 0.8137\n",
            "Epoch 2/5\n",
            "1018/1018 [==============================] - 8s 8ms/step - loss: 0.3564 - accuracy: 0.8336\n",
            "Epoch 3/5\n",
            "1018/1018 [==============================] - 8s 8ms/step - loss: 0.3488 - accuracy: 0.8386\n",
            "Epoch 4/5\n",
            "1018/1018 [==============================] - 8s 8ms/step - loss: 0.3559 - accuracy: 0.8392\n",
            "Epoch 5/5\n",
            "1018/1018 [==============================] - 9s 9ms/step - loss: 0.3400 - accuracy: 0.8442\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f10d62f9790>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model.fit(training_ds, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgKKliXRZG_G"
      },
      "source": [
        "##Evaluate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgNd5DdU7TEW"
      },
      "source": [
        "Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8eGHVkmI5LBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c597a3c9-b903-4f42-c56d-f74a7e9b80dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "509/509 [==============================] - 6s 12ms/step - loss: 0.3555 - accuracy: 0.8249\n",
            "Accuracy 0.8249171\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(eval_ds)\n",
        "print(\"Accuracy\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBIWoGMP7bj1"
      },
      "source": [
        "Evaluate a couple of random samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aMou1t1xngXP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d848eb90-e594-4b7b-9a90-cf5fe5b20558"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.67360896],\n",
              "       [0.33105135]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "sample_x = {\n",
        "    'age' : np.array([56, 36]), \n",
        "    'workclass': np.array(['Local-gov', 'Private']), \n",
        "    'education': np.array(['Bachelors', 'Bachelors']), \n",
        "    'marital_status': np.array(['Married-civ-spouse', 'Married-civ-spouse']), \n",
        "    'occupation': np.array(['Tech-support', 'Other-service']), \n",
        "    'relationship': np.array(['Husband', 'Husband']), \n",
        "    'race': np.array(['White', 'Black']), \n",
        "    'gender': np.array(['Male', 'Male']), \n",
        "    'capital_gain': np.array([0, 7298]), \n",
        "    'capital_loss': np.array([0, 0]), \n",
        "    'hours_per_week': np.array([40, 36]), \n",
        "    'native_country': np.array(['United-States', 'United-States'])\n",
        "  }\n",
        "\n",
        "model.predict(sample_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhNtHfuxCGVy"
      },
      "source": [
        "## Resources\n",
        "\n",
        "* [Google Cloud BigQuery Overview](https://github.com/tensorflow/io/blob/master/tensorflow_io/bigquery/README.md)\n",
        "* [Training and prediction with Keras in AI Platform](https://colab.sandbox.google.com/github/GoogleCloudPlatform/cloudml-samples/blob/master/notebooks/tensorflow/getting-started-keras.ipynb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "bigquery.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}